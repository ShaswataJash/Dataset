{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "115aePpvdxaPthofW3Z412dd1e1QrMS9X",
      "authorship_tag": "ABX9TyMbxkzoFsCiipY6up69hGuU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaswataJash/LargeDatasetHandling/blob/master/Incremental_min_max_calculation_for_large_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uname -a\n",
        "!python --version"
      ],
      "metadata": {
        "id": "ABv1EfkN9JmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ce9021-9ae2-44b2-b832-6435344c6a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linux 5f5882c553b5 5.10.147+ #1 SMP Sat Dec 10 16:00:40 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Python 3.9.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ2e2iY8Sb7I",
        "outputId": "52262a98-6f14-4db7-fa2c-44dd61589c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!df -h /dev/shm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNOT1CsxHpwS",
        "outputId": "6bf249ca-8d78-4836-f1d7-655a3a478a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "shm             5.7G     0  5.7G   0% /dev/shm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/7878707/how-to-unmount-a-busy-device\n",
        "#for python multipprocessor, data across child process and main process are being shared through shared memory\n",
        "#for pytorch Dataloader, shared memory requirement can be quite high\n",
        "!sudo umount -l /dev/shm/ && sudo mount -t tmpfs -o rw,nosuid,nodev,noexec,relatime,size=9G shm /dev/shm"
      ],
      "metadata": {
        "id": "ik3UCDFtQnAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#refer: https://numpy.org/doc/stable/reference/global_state.html#madvise-hugepage-on-linux\n",
        "!cat /sys/kernel/mm/transparent_hugepage/enabled\n",
        "!cat /sys/kernel/mm/transparent_hugepage/defrag\n",
        "!cat /sys/kernel/mm/transparent_hugepage/use_zero_page\n",
        "!cat /sys/kernel/mm/transparent_hugepage/hpage_pmd_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4r4JFDEV7SM",
        "outputId": "e0e9ba28-056c-4791-85b5-9f5f33c04461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "always [madvise] never\n",
            "always defer defer+madvise [madvise] never\n",
            "1\n",
            "2097152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env"
      ],
      "metadata": {
        "id": "dbm8LTIEZS5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/37890898/how-to-set-env-variable-in-jupyter-notebook\n",
        "%env NUMPY_MADVISE_HUGEPAGE=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxWhNXIJZec5",
        "outputId": "9d3b9e6f-c0aa-4560-d96f-aff476c8a8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: NUMPY_MADVISE_HUGEPAGE=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Determine total availiable GPU memory"
      ],
      "metadata": {
        "id": "gBnAWDn9AY-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ref: https://stackoverflow.com/questions/59567226/how-to-programmatically-determine-available-gpu-memory-with-tensorflow\n",
        "import subprocess as sp\n",
        "import os\n",
        "def get_gpu_memory():\n",
        "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
        "    try:\n",
        "        memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
        "        memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
        "        return memory_free_values[0] * 1024 * 1024 # memory_free_values[0] is in MB, thus converting into bytes\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return -1"
      ],
      "metadata": {
        "id": "7j4O2tiG7PD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#downloading kaggle competitions files"
      ],
      "metadata": {
        "id": "pGV5IETWGxjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle==1.5.12"
      ],
      "metadata": {
        "id": "fmJBzixSfGEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmxlJwRZbYtA"
      },
      "outputs": [],
      "source": [
        "%%python\n",
        "\n",
        "import sys\n",
        "import logging\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s:%(levelname)s:%(message)s')\n",
        "logger = logging.getLogger('my_logger')\n",
        "#handling of kaggle interaction\n",
        "try:\n",
        "    os.environ[\"KAGGLE_CONFIG_DIR\"] = '/home' #kaggle.json file should be uploaded to /home location before executing this cell\n",
        "    kaggle_write_cmd = \"kaggle competitions download -c open-problems-multimodal\"\n",
        "    kaggle_write_call = subprocess.run(kaggle_write_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    logger.info(kaggle_write_call.stdout)\n",
        "    if kaggle_write_call.returncode != 0:\n",
        "        logger.error(\"Error in kaggle download, errorcode=%s\", kaggle_write_call.returncode)\n",
        "        sys.stdout.flush()\n",
        "        sys.exit(\"Forceful exit as kaggle download returned error\")\n",
        "except BaseException as err:\n",
        "    logger.error(\"kaggle download related error\", exc_info=True)\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while kaggle download\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/colab_exp_result/kaggle_data\n",
        "!unzip /content/open-problems-multimodal.zip -d /content/drive/MyDrive/colab_exp_result/kaggle_data"
      ],
      "metadata": {
        "id": "TT-gWlUYCG7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can mount Google drive in colab and can copy the kaggle competitions files there. This will help not to run kaggle download code everytime before start of the notebook - it can save lot of time. Instead, everytime we can directly copy the contents from drive into the local filesystem of the underneath VM hosting the notebook."
      ],
      "metadata": {
        "id": "-tmhnBezqNTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup cp /content/drive/MyDrive/colab_exp_result/kaggle_data/* /mnt &"
      ],
      "metadata": {
        "id": "e_jqgtMpmsLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea8ab867-4e40-4eeb-e148-baf0fdc11287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /mnt"
      ],
      "metadata": {
        "id": "C77_Ud1igRen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c33407-246b-48dd-98e7-c1b86174a539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 28181076\n",
            "-rw------- 1 root root  2418406934 Apr 18 03:25 evaluation_ids.csv\n",
            "-rw------- 1 root root      551250 Apr 18 03:25 max_cite_inputs.txt\n",
            "-rw------- 1 root root     5723550 Apr 18 03:26 max_multi_inputs.txt\n",
            "-rw------- 1 root root      234920 Apr 18 03:26 metadata_cite_day_2_donor_27678.csv\n",
            "-rw------- 1 root root     9770334 Apr 18 03:26 metadata.csv\n",
            "-rw------- 1 root root      551250 Apr 18 03:26 min_cite_inputs.txt\n",
            "-rw------- 1 root root     5723550 Apr 18 03:26 min_multi_inputs.txt\n",
            "-rw------- 1 root root   843563244 Apr 18 03:26 sample_submission.csv\n",
            "-rw------- 1 root root   307964530 Apr 18 03:26 test_cite_inputs_day_2_donor_27678.h5\n",
            "-rw------- 1 root root  1704565845 Apr 18 03:26 test_cite_inputs.h5\n",
            "-rw------- 1 root root  6473530657 Apr 18 03:27 test_multi_inputs.h5\n",
            "-rw------- 1 root root  2498128492 Apr 18 03:28 train_cite_inputs.h5\n",
            "-rw------- 1 root root    38539123 Apr 18 03:28 train_cite_targets.h5\n",
            "-rw------- 1 root root 11334840656 Apr 18 03:30 train_multi_inputs.h5\n",
            "-rw------- 1 root root  3215261538 Apr 18 03:30 train_multi_targets.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installation of required software packages"
      ],
      "metadata": {
        "id": "4Pu-jAqrasbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py==3.8.0"
      ],
      "metadata": {
        "id": "U5wZBh7dyDZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be9ee2ee-4df9-41c0-879c-bbd1f4d6c663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py==3.8.0 in /usr/local/lib/python3.9/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.9/dist-packages (from h5py==3.8.0) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ref: https://docs.h5py.org/en/stable/mpi.html\n",
        "#check whether parallel version of h5py is availiable\n",
        "!h5cc -showconfig"
      ],
      "metadata": {
        "id": "TN3pxs5zhvGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdf5plugin~=2.0"
      ],
      "metadata": {
        "id": "GsjShaVYyIg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b073433-8c83-4202-d01f-dac4b31b0c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hdf5plugin~=2.0\n",
            "  Downloading hdf5plugin-2.3.2-py2.py3-none-manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (from hdf5plugin~=2.0) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.9/dist-packages (from h5py->hdf5plugin~=2.0) (1.22.4)\n",
            "Installing collected packages: hdf5plugin\n",
            "Successfully installed hdf5plugin-2.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HDF5 handling common code"
      ],
      "metadata": {
        "id": "upQT9S7Ea3tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import hdf5plugin #without importing this, decompression will not happen by h5py\n",
        "def get_hdf5_dataset_value_key(hdf5_file, debug = 0):\n",
        "    groups = []\n",
        "    def node_visit(name):\n",
        "        groups.append(name)\n",
        "    \n",
        "    hdf5_file.visit(node_visit)\n",
        "    if debug>0: print(hdf5_file, groups)\n",
        "    \n",
        "    for g in groups:\n",
        "        shape = hdf5_file[g].shape if isinstance(hdf5_file[g], h5py._hl.dataset.Dataset) else None\n",
        "        if debug>0: print(g, type(hdf5_file[g]), shape)\n",
        "        if (not shape is None) and (len(shape) == 2):\n",
        "            return g\n",
        "    \n",
        "    return None\n",
        "\n",
        "def get_hdf5_dataset_with_specific_shape(hdf5_file, size, debug = 0):\n",
        "    groups = []\n",
        "    def node_visit(name):\n",
        "        groups.append(name)\n",
        "    \n",
        "    hdf5_file.visit(node_visit)\n",
        "    if debug>0: print(hdf5_file, groups)\n",
        "    \n",
        "    for g in groups:\n",
        "        shape = hdf5_file[g].shape if isinstance(hdf5_file[g], h5py._hl.dataset.Dataset) else None\n",
        "        if debug>0: print(g, type(hdf5_file[g]), shape)\n",
        "        if (not shape is None) and (len(shape) == 1) and (shape[0] == size):\n",
        "            return g\n",
        "    \n",
        "    return None\n",
        "\n",
        "def get_hdf5_info(hdf5_file):\n",
        "    print('root-group file-object name:', hdf5_file.name)\n",
        "    def print_keys(gr, level):\n",
        "        keys = list(gr.keys())\n",
        "        for k in keys:\n",
        "            \n",
        "            if isinstance(gr[k], h5py._hl.group.Group):\n",
        "                print('->'*level, k, gr[k])\n",
        "                print_keys(gr[k], level + 1)\n",
        "            elif isinstance(gr[k], h5py._hl.dataset.Dataset):\n",
        "                print('->'*level, k, gr[k], 'dtype=', gr[k].dtype , 'size=', gr[k].size, 'nbytes=', gr[k].nbytes, \n",
        "                      'maxshape=', gr[k].maxshape, 'chunks=', gr[k].chunks)\n",
        "\n",
        "    print_keys(hdf5_file, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "rAqMEPw4yQwL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import hdf5plugin #without importing this, decompression will not happen by h5py\n",
        "print('============= TRAIN MULTI INPUT ====================')\n",
        "train_multi_input_file = h5py.File('/mnt/train_multi_inputs.h5') # HDF5 file\n",
        "get_hdf5_info(train_multi_input_file)\n",
        "train_multi_input_file.close()\n",
        "del train_multi_input_file\n",
        "print('============= TEST MULTI INPUT ====================')\n",
        "test_multi_input_file = h5py.File('/mnt/test_multi_inputs.h5') # HDF5 file\n",
        "get_hdf5_info(test_multi_input_file)\n",
        "test_multi_input_file.close()\n",
        "del test_multi_input_file"
      ],
      "metadata": {
        "id": "8m9Y6UQNp5o9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb8942a-8f1e-4670-e2c7-c4ec42001dc1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= TRAIN MULTI INPUT ====================\n",
            "root-group file-object name: /\n",
            "-> train_multi_inputs <HDF5 group \"/train_multi_inputs\" (4 members)>\n",
            "->-> axis0 <HDF5 dataset \"axis0\": shape (228942,), type \"|S26\"> dtype= |S26 size= 228942 nbytes= 5952492 maxshape= (228942,) chunks= (2520,)\n",
            "->-> axis1 <HDF5 dataset \"axis1\": shape (105942,), type \"|S12\"> dtype= |S12 size= 105942 nbytes= 1271304 maxshape= (105942,) chunks= (5461,)\n",
            "->-> block0_items <HDF5 dataset \"block0_items\": shape (228942,), type \"|S26\"> dtype= |S26 size= 228942 nbytes= 5952492 maxshape= (228942,) chunks= (2520,)\n",
            "->-> block0_values <HDF5 dataset \"block0_values\": shape (105942, 228942), type \"<f4\"> dtype= float32 size= 24254573364 nbytes= 97018293456 maxshape= (105942, 228942) chunks= (1, 228942)\n",
            "============= TEST MULTI INPUT ====================\n",
            "root-group file-object name: /\n",
            "-> test_multi_inputs <HDF5 group \"/test_multi_inputs\" (4 members)>\n",
            "->-> axis0 <HDF5 dataset \"axis0\": shape (228942,), type \"|S26\"> dtype= |S26 size= 228942 nbytes= 5952492 maxshape= (228942,) chunks= (2520,)\n",
            "->-> axis1 <HDF5 dataset \"axis1\": shape (55935,), type \"|S12\"> dtype= |S12 size= 55935 nbytes= 671220 maxshape= (55935,) chunks= (5461,)\n",
            "->-> block0_items <HDF5 dataset \"block0_items\": shape (228942,), type \"|S26\"> dtype= |S26 size= 228942 nbytes= 5952492 maxshape= (228942,) chunks= (2520,)\n",
            "->-> block0_values <HDF5 dataset \"block0_values\": shape (55935, 228942), type \"<f4\"> dtype= float32 size= 12805870770 nbytes= 51223483080 maxshape= (55935, 228942) chunks= (1, 228942)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import hdf5plugin #without importing this, decompression will not happen by h5py\n",
        "train_mult_input_file = h5py.File('/mnt/train_multi_inputs.h5') # HDF5 file\n",
        "hdf5_input_key = get_hdf5_dataset_value_key(train_mult_input_file, debug=1)"
      ],
      "metadata": {
        "id": "RsVDw1_dlFFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf3f54f1-b439-4762-d9f9-b874911a8a8d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<HDF5 file \"train_multi_inputs.h5\" (mode r)> ['train_multi_inputs', 'train_multi_inputs/axis0', 'train_multi_inputs/axis1', 'train_multi_inputs/block0_items', 'train_multi_inputs/block0_values']\n",
            "train_multi_inputs <class 'h5py._hl.group.Group'> None\n",
            "train_multi_inputs/axis0 <class 'h5py._hl.dataset.Dataset'> (228942,)\n",
            "train_multi_inputs/axis1 <class 'h5py._hl.dataset.Dataset'> (105942,)\n",
            "train_multi_inputs/block0_items <class 'h5py._hl.dataset.Dataset'> (228942,)\n",
            "train_multi_inputs/block0_values <class 'h5py._hl.dataset.Dataset'> (105942, 228942)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hdf5_col_name_key = get_hdf5_dataset_with_specific_shape(train_mult_input_file, 228942, debug=1)\n",
        "cols = train_mult_input_file[hdf5_col_name_key]\n",
        "print(cols.shape)\n",
        "from tqdm import tqdm\n",
        "col_name = []\n",
        "for c_id in tqdm(range(cols.shape[0])):\n",
        "    col_name.append(str(cols[c_id], 'UTF-8'))"
      ],
      "metadata": {
        "id": "5E16JH80v7FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   https://luis-sena.medium.com/sharing-big-numpy-arrays-across-python-processes-abf0dc2a0ab2 (why ray with shared object store is best sol)\n",
        "*   Ref: https://towardsdatascience.com/histogram-on-function-space-4a710241f026\n",
        "*   Ref: https://stackoverflow.com/questions/71844846/is-there-a-faster-way-to-get-correlation-coefficents (fast corr-coef)\n",
        "\n"
      ],
      "metadata": {
        "id": "4aadQ-lstUfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global min and max determination of the raw-inputs (will be used for min-max normalization of the data)"
      ],
      "metadata": {
        "id": "gbP33e4B293Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile rawInputDataset.py\n",
        "#read: https://medium.com/@grvsinghal/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac\n",
        "#The above medium article explains why Dataset related class (or the function which is being called from multiprocessor)\n",
        "# has to be defined in separate python class for jupyter notebook\n",
        "import os\n",
        "import traceback\n",
        "import numpy as np\n",
        "import h5py\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import h5py\n",
        "import hdf5plugin #without importing this, decompression will not happen by h5py\n",
        "\n",
        "def get_hdf5_dataset_value_key(hdf5_file, debug = 0):\n",
        "    groups = []\n",
        "    def node_visit(name):\n",
        "        groups.append(name)\n",
        "    \n",
        "    hdf5_file.visit(node_visit)\n",
        "    if debug>0: print(hdf5_file, groups)\n",
        "    \n",
        "    for g in groups:\n",
        "        shape = hdf5_file[g].shape if isinstance(hdf5_file[g], h5py._hl.dataset.Dataset) else None\n",
        "        if debug>0: print(g, type(hdf5_file[g]), shape)\n",
        "        if (not shape is None) and (len(shape) == 2):\n",
        "            return g\n",
        "    \n",
        "    return None\n",
        "\n",
        "class RawInputDataset(Dataset):\n",
        "\n",
        "    def __init__(self, hdf5_input_path, len, print_lock, debug=0):\n",
        "        self.hdf5_input_path = hdf5_input_path\n",
        "        self.len = len\n",
        "        self.print_lock = print_lock\n",
        "        self.hdf5_per_process = {}\n",
        "        self.stat_per_process = {}\n",
        "        self.debug = debug\n",
        "\n",
        "    def __getstate__(self):\n",
        "        my_mod_dic = self.__dict__.copy()\n",
        "        #h5py object are not pickable, thus we are removing them from object state while pickling\n",
        "        my_mod_dic['hdf5_per_process'] = {}\n",
        "        with self.print_lock:\n",
        "            if self.debug>0: print('pid=', os. getpid(), 'pickled object state:', my_mod_dic)\n",
        "        return my_mod_dic\n",
        "    \n",
        "    '''\n",
        "    def __setstate__(self, d):\n",
        "        self.hdf5_input_path = d['hdf5_input_path']\n",
        "        self.len = d['len']\n",
        "        self.print_lock = d['print_lock']\n",
        "        self.hdf5_per_process = {}\n",
        "        self.debug = d['debug']\n",
        "        with self.print_lock:\n",
        "            if self.debug>0: print('pid=', os. getpid(), 'unpickled object state:', self.__dict__)\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __len__(self): return self.len\n",
        "\n",
        "    def internal_initialize(self, batch_size=1):\n",
        "        \n",
        "        if os.getpid() in self.hdf5_per_process:\n",
        "            return\n",
        "        hdf5_input = h5py.File(self.hdf5_input_path, 'r', driver='stdio')\n",
        "        hdf5_input_key = get_hdf5_dataset_value_key(hdf5_input)\n",
        "        len = hdf5_input[hdf5_input_key].shape[0]\n",
        "        assert len == self.len\n",
        "        input = None\n",
        "        if batch_size > 1:\n",
        "            input = np.zeros((batch_size,hdf5_input[hdf5_input_key].shape[1]), dtype=hdf5_input[hdf5_input_key].dtype)           \n",
        "        self.hdf5_per_process[os.getpid()] = (hdf5_input[hdf5_input_key], input)\n",
        "        self.stat_per_process[os.getpid()] = []\n",
        "        if self.debug>0:\n",
        "            with self.print_lock:\n",
        "                print('pid=', os. getpid(), 'internal_initialize:', hdf5_input, hdf5_input_key, hdf5_input[hdf5_input_key], flush=True)\n",
        "                if not (input is None):\n",
        "                    print('precreated numpy arr input:', input.shape)\n",
        "    \n",
        "\n",
        "    def __getitem__(self, row):\n",
        "        try:\n",
        "            self.internal_initialize()\n",
        "            assert row < self.len\n",
        "            (hdf5_dataset,  _) = self.hdf5_per_process[os.getpid()]\n",
        "            if self.debug>0: \n",
        "                #ref: https://stackoverflow.com/questions/56364119/managed-dict-of-list-not-updated-in-multiprocessing-when-using-operator\n",
        "                l = self.stat_per_process[os.getpid()]\n",
        "                l.append(row)\n",
        "                self.stat_per_process[os.getpid()] = l\n",
        "            input = hdf5_dataset[row:row+1]\n",
        "            #print('type of input=', type(input) , 'shape=', input.shape, flush=True)\n",
        "            numpy_arr = np.ravel(input)\n",
        "            return torch.from_numpy(numpy_arr).detach()\n",
        "        except Exception as e:\n",
        "            print('Exception occurred in __getitem__:', e)\n",
        "            traceback.print_exc()\n",
        "            \n",
        "        return None\n",
        "\n",
        "    def get_batch(self, starting_row, batch_size):\n",
        "        try:\n",
        "            self.internal_initialize(batch_size)\n",
        "            assert starting_row < self.len\n",
        "            (hdf5_dataset, input) = self.hdf5_per_process[os.getpid()]\n",
        "            end_row = min(starting_row + batch_size, self.len)\n",
        "            if self.debug>0:\n",
        "                #ref: https://stackoverflow.com/questions/56364119/managed-dict-of-list-not-updated-in-multiprocessing-when-using-operator\n",
        "                l = self.stat_per_process[os.getpid()]\n",
        "                l.extend(range(starting_row, end_row))\n",
        "                self.stat_per_process[os.getpid()] = l\n",
        "            \n",
        "            #input = hdf5_dataset[starting_row:end_row]\n",
        "            \n",
        "            assert input.shape[0] <= batch_size\n",
        "            if input.shape[0] != (end_row - starting_row): #will happen for the last batch\n",
        "                input = np.zeros(((end_row - starting_row),hdf5_dataset.shape[1]), dtype=hdf5_dataset.dtype)\n",
        "            hdf5_dataset.read_direct(input, source_sel=np.s_[starting_row:end_row,:], dest_sel=None)\n",
        "            \n",
        "            #print('type of input=', type(input) , 'shape=', input.shape, flush=True)\n",
        "            return torch.from_numpy(input).detach()\n",
        "        except Exception as e:\n",
        "            print('Exception occurred in get_batch():', e)\n",
        "            traceback.print_exc()\n",
        "            \n",
        "        return None\n",
        "\n",
        "    def reset_consumed_record(self):\n",
        "        if self.debug <= 0:\n",
        "           return\n",
        "        \n",
        "        with self.print_lock:\n",
        "            for key in self.stat_per_process.keys():\n",
        "                record_consumed = self.stat_per_process[key]\n",
        "                print('pid=', key, 'consumed element count = ', len(record_consumed))\n",
        "                if self.debug>2: print(record_consumed)\n",
        "                record_consumed.clear()    \n",
        "\n",
        "    def __del__(self):\n",
        "        \n",
        "        for key in self.hdf5_per_process.keys():\n",
        "            (hdf5_input,  hdf5_input_key) = self.hdf5_per_process[key]\n",
        "            del hdf5_input_key\n",
        "            hdf5_input.close()\n",
        "            del hdf5_input\n"
      ],
      "metadata": {
        "id": "kh15dnhSbgTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08dc38b-0018-400c-8344-431c03152351"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting rawInputDataset.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import traceback\n",
        "import psutil\n",
        "class OnlineMinMaxSequential:\n",
        "    def __init__(self, iteration_count=-1):\n",
        "        self.iteration_count = iteration_count\n",
        "\n",
        "    def last_call_mem_req(self):\n",
        "        return self.gpu_req_mem, self.ram_req_mem\n",
        "\n",
        "    def __call__(self, my_dataset, batch_size, gc_call_per_iteration, result_save_path_max = None, result_save_path_min = None):\n",
        "        starting_gpu_mem = get_gpu_memory()\n",
        "        starting_ram = psutil.virtual_memory().available\n",
        "\n",
        "        global_min = None\n",
        "        global_max = None\n",
        "\n",
        "        my_dataset.reset_consumed_record()\n",
        "        for batch_count,starting_row in tqdm(enumerate(range(0, len(my_dataset), batch_size), start=1)):\n",
        "            try:\n",
        "                data = my_dataset.get_batch(starting_row, batch_size)\n",
        "                #data = data.to(torch.device(\"cuda:0\"))\n",
        "                \n",
        "                local_min = torch.min(data, dim=0)[0] #we have to find min for each col (so reduction of dim=0)\n",
        "                local_max = torch.max(data, dim=0)[0] #we have to find max for each col (so reduction of dim=0)\n",
        "                if not (global_min is None):\n",
        "                    global_min = torch.minimum(global_min, local_min)\n",
        "                    global_max = torch.maximum(global_max, local_max)\n",
        "                else:\n",
        "                    global_min = local_min\n",
        "                    global_max = local_max\n",
        "                    if not(result_save_path_max is None): np.savetxt(result_save_path_max, global_max.numpy())\n",
        "                    if not(result_save_path_min is None): np.savetxt(result_save_path_min, global_min.numpy())\n",
        "                \n",
        "                my_dataset.reset_consumed_record()\n",
        "                if gc_call_per_iteration:\n",
        "                    while(gc.collect() > 0): pass #clean the memory as much as possible\n",
        "                if (self.iteration_count > 0) and (batch_count >= self.iteration_count ):\n",
        "                    break\n",
        "            \n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                traceback.print_exc()\n",
        "\n",
        "        end_gpu_mem = get_gpu_memory()\n",
        "        end_ram = psutil.virtual_memory().available\n",
        "\n",
        "        self.gpu_req_mem = starting_gpu_mem - end_gpu_mem\n",
        "        self.ram_req_mem = starting_ram - end_ram\n",
        "\n",
        "        return global_max, global_min"
      ],
      "metadata": {
        "id": "TyK5wUPzoKrL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import traceback\n",
        "import psutil\n",
        "\n",
        "def my_worker_init_fn(worker_id):\n",
        "    worker_info = torch.utils.data.get_worker_info()\n",
        "    print('pid=', os. getpid(), 'worker-info:', worker_info)\n",
        "    assert worker_info.id == worker_id\n",
        "    raw_input_dataset = worker_info.dataset\n",
        "    print(raw_input_dataset)\n",
        "    #raw_input_dataset.internal_initialize()\n",
        "\n",
        "class OnlineMinMax:\n",
        "    def __init__(self, worker_count, iteration_count=-1):\n",
        "        self.workers = worker_count\n",
        "        self.iteration_count = iteration_count\n",
        "\n",
        "    def last_call_mem_req(self):\n",
        "        return self.gpu_req_mem, self.ram_req_mem\n",
        "\n",
        "    def __call__(self, my_dataset, batch_size, gc_call_per_iteration, result_save_path_max = None, result_save_path_min = None):\n",
        "        starting_gpu_mem = get_gpu_memory()\n",
        "        starting_ram = psutil.virtual_memory().available\n",
        "        my_dataset.reset_consumed_record()\n",
        "        loader = DataLoader(dataset=my_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=False,\n",
        "                                num_workers=self.workers,\n",
        "                                prefetch_factor=None,\n",
        "                                #worker_init_fn=my_worker_init_fn,\n",
        "                                #multiprocessing_context='spawn',\n",
        "                                #timeout=300,\n",
        "                                #num_workers=1,\n",
        "                                #pin_memory=True,\n",
        "                                )\n",
        "        \n",
        "        global_min = None\n",
        "        global_max = None\n",
        "        for batch_count,data in tqdm(enumerate(loader, start=1)):\n",
        "            try:\n",
        "                #data = data.to(torch.device(\"cuda:0\"))\n",
        "                #print(data.shape)\n",
        "                local_min = torch.min(data, dim=0)[0] #we have to find min for each col (so reduction of dim=0)\n",
        "                local_max = torch.max(data, dim=0)[0] #we have to find max for each col (so reduction of dim=0)\n",
        "                if not (global_min is None):\n",
        "                    global_min = torch.minimum(global_min, local_min)\n",
        "                    global_max = torch.maximum(global_max, local_max)\n",
        "                else:\n",
        "                    global_min = local_min\n",
        "                    global_max = local_max\n",
        "                    if not(result_save_path_max is None): np.savetxt(result_save_path_max, global_max.numpy())\n",
        "                    if not(result_save_path_min is None): np.savetxt(result_save_path_min, global_min.numpy())\n",
        "\n",
        "                if gc_call_per_iteration:\n",
        "                    while(gc.collect() > 0): pass #clean the memory as much as possible\n",
        "                if (self.iteration_count > 0) and (batch_count >= self.iteration_count ):\n",
        "                    break\n",
        "            \n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                traceback.print_exc()\n",
        "\n",
        "        end_gpu_mem = get_gpu_memory()\n",
        "        end_ram = psutil.virtual_memory().available\n",
        "\n",
        "        self.gpu_req_mem = starting_gpu_mem - end_gpu_mem\n",
        "        self.ram_req_mem = starting_ram - end_ram\n",
        "\n",
        "        del loader\n",
        "        return global_max, global_min"
      ],
      "metadata": {
        "id": "s9t3iG76P0IY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/2918898/prevent-python-from-caching-the-imported-modules\n",
        "#we have to ensure 'rawInputDataset' module is reloaded everytime (helps us to continously change source code of rawInputDataset.py file)\n",
        "%load_ext autoreload"
      ],
      "metadata": {
        "id": "MSHI-U1lQnCJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%autoreload 2"
      ],
      "metadata": {
        "id": "Wy-ZSdr2Qxi5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install memray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAbGnW3CAzm5",
        "outputId": "84ab39b7-cad5-4d33-fd9b-7ef360196ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting memray\n",
            "  Downloading memray-1.7.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=11.2.0 in /usr/local/lib/python3.9/dist-packages (from memray) (13.3.3)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.9/dist-packages (from memray) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2>=2.9->memray) (2.1.2)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich>=11.2.0->memray) (2.14.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich>=11.2.0->memray) (2.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.2.0->memray) (0.1.2)\n",
            "Installing collected packages: memray\n",
            "Successfully installed memray-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext memray"
      ],
      "metadata": {
        "id": "vgv-oWy1A_eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%memray_flamegraph --trace-python-allocators --follow-fork --native --leaks \n",
        "import math\n",
        "from rawInputDataset import RawInputDataset\n",
        "import traceback\n",
        "import multiprocessing\n",
        "from multiprocessing import current_process\n",
        "from threading import current_thread\n",
        "import inspect\n",
        "import gc\n",
        "import os\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "\n",
        "#https://superfastpython.com/multiprocessing-pool-initializer/#Example_of_Accessing_an_Initialized_Variable_in_a_Worker\n",
        "def my_custom_init(arg):\n",
        "    # declare global variable\n",
        "    global custom_data\n",
        "    # assign the global variable\n",
        "    custom_data = arg\n",
        "    # get the current process\n",
        "    process = current_process()\n",
        "    # get the current thread\n",
        "    thread = current_thread()\n",
        "    # report a message\n",
        "    with custom_data.print_lock:\n",
        "        print(f'Initializing worker pid={os. getpid()} process={process.name}, thread={thread.name}, with={custom_data}', flush=True)\n",
        "\n",
        "\n",
        "def my_custom_min_max_finding_job(arg):\n",
        "    global custom_data\n",
        "    (starting_row, my_batch_size) = arg\n",
        "    data = custom_data.get_batch(starting_row, my_batch_size)\n",
        "    #data = data.to(torch.device(\"cuda:0\"))\n",
        "    local_min = torch.min(data, dim=0)[0] #we have to find min for each col (so reduction of dim=0)\n",
        "    local_max = torch.max(data, dim=0)[0] #we have to find max for each col (so reduction of dim=0)    \n",
        "    return (local_min, local_max)\n",
        "\n",
        "def find_min_max(manager, algo_type='SEQ'):\n",
        "    print_lock = manager.Lock() #https://superfastpython.com/multiprocessing-pool-mutex-lock/\n",
        "    #print_lock = tqdm.get_lock()\n",
        "    try:\n",
        "        hdf5_input_file_path = '/mnt/train_multi_inputs.h5'\n",
        "        hdf5_input = h5py.File(hdf5_input_file_path, 'r')\n",
        "        hdf5_input_key = get_hdf5_dataset_value_key(hdf5_input)\n",
        "        data_len = hdf5_input[hdf5_input_key].shape[0]\n",
        "        first_elem = hdf5_input[hdf5_input_key][0]\n",
        "        elem_size_in_bytes = first_elem.size * first_elem.itemsize\n",
        "        print('first_elem.size:', first_elem.size, 'first_elem.itemsize:', first_elem.itemsize, 'elem_size_in_bytes:', elem_size_in_bytes)\n",
        "        hdf5_input.close()\n",
        "        del hdf5_input\n",
        "\n",
        "        my_dataset = RawInputDataset(hdf5_input_file_path, data_len, print_lock, debug=1)\n",
        "        print(my_dataset, 'len=', len(my_dataset))\n",
        "        #optimal_batch_size = math.floor((1024 * 1024 * 1024) / elem_size_in_bytes) #max 1GB of numpy array \n",
        "        optimal_batch_size = math.floor((20 * 1024 * 1024) / elem_size_in_bytes) #max 20MB of numpy array \n",
        "        print('optimal_batch_size:', optimal_batch_size)\n",
        "\n",
        "        while(gc.collect() > 0): pass #clean the memory as much as possible\n",
        "\n",
        "        if algo_type=='SEQ':\n",
        "            olmm = OnlineMinMaxSequential()\n",
        "            max, min = olmm(my_dataset, optimal_batch_size, gc_call_per_iteration=True)\n",
        "        elif algo_type=='TORCH_PARALLEL':\n",
        "            olmm = OnlineMinMax(os.cpu_count() * 2)\n",
        "            max, min = olmm(my_dataset, optimal_batch_size, gc_call_per_iteration=True)\n",
        "        elif algo_type=='CUSTOM_PARALLEL':\n",
        "            my_dataset.stat_per_process = manager.dict()\n",
        "            work_arg = []\n",
        "            for s_row in range(0, len(my_dataset), optimal_batch_size):\n",
        "            #for s_row in range(0, 6 * optimal_batch_size, optimal_batch_size):\n",
        "                work_arg.append((s_row, optimal_batch_size))\n",
        "            print(\"total task required:\", len(work_arg))\n",
        "            with  multiprocessing.Pool(os.cpu_count(), initializer=my_custom_init, initargs=(my_dataset,)) as p:\n",
        "                #For very long iterables using a large value for chunksize can make the job complete much faster than using the default value of 1 \n",
        "                #(refer: https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.imap_unordered)\n",
        "                result = p.imap_unordered(my_custom_min_max_finding_job, work_arg, chunksize=1)\n",
        "                min = None\n",
        "                max = None\n",
        "                for r in tqdm(result):\n",
        "                    if not (min is None):\n",
        "                        min = torch.minimum(min, r[0])\n",
        "                        max = torch.maximum(max, r[1])\n",
        "                    else:\n",
        "                        min = r[0]\n",
        "                        max = r[1]\n",
        "\n",
        "                # close the process pool\n",
        "                p.close()\n",
        "                # wait for all tasks to complete\n",
        "                p.join()\n",
        "            \n",
        "            my_dataset.reset_consumed_record()\n",
        "            \n",
        "        print('max.shape:', max.shape, 'min.shape:', min.shape)\n",
        "        return max, min\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        del my_dataset\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('MAIN pid=', os. getpid())\n",
        "\n",
        "    with multiprocessing.Manager() as manager:\n",
        "        find_min_max(manager, algo_type='CUSTOM_PARALLEL')\n",
        "        #find_min_max(manager, algo_type='SEQ')\n",
        "        #find_min_max(manager, algo_type='TORCH_PARALLEL')\n",
        "    gc.collect()\n",
        "    print(gc.get_stats())"
      ],
      "metadata": {
        "id": "t6Lcq3D7qB__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4cf9e5a-d33f-4128-e69e-d3d2aa366c28"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAIN pid= 126087\n",
            "first_elem.size: 228942 first_elem.itemsize: 4 elem_size_in_bytes: 915768\n",
            "<rawInputDataset.RawInputDataset object at 0x7f9561fb6f70> len= 105942\n",
            "optimal_batch_size: 22\n",
            "total task required: 4816\n",
            "Initializing worker pid=127233 process=ForkPoolWorker-2, thread=MainThread, with=<rawInputDataset.RawInputDataset object at 0x7f9561fb6f70>\n",
            "Initializing worker pid=127239 process=ForkPoolWorker-3, thread=MainThread, with=<rawInputDataset.RawInputDataset object at 0x7f9561fb6f70>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pid= 127233 internal_initialize: <HDF5 file \"train_multi_inputs.h5\" (mode r)> train_multi_inputs/block0_values <HDF5 dataset \"block0_values\": shape (105942, 228942), type \"<f4\">\n",
            "precreated numpy arr input: (22, 228942)\n",
            "pid= 127239 internal_initialize: <HDF5 file \"train_multi_inputs.h5\" (mode r)> train_multi_inputs/block0_values <HDF5 dataset \"block0_values\": shape (105942, 228942), type \"<f4\">\n",
            "precreated numpy arr input: (22, 228942)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4816it [02:41, 29.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pid= 127233 consumed element count =  52954\n",
            "pid= 127239 consumed element count =  52988\n",
            "max.shape: torch.Size([228942]) min.shape: torch.Size([228942])\n",
            "[{'collections': 583, 'collected': 3009, 'uncollectable': 0}, {'collections': 52, 'collected': 654, 'uncollectable': 0}, {'collections': 6, 'collected': 0, 'uncollectable': 0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt('/content/drive/MyDrive/colab_exp_result/kaggle_data/max_multi_inputs.txt', max_m.numpy())\n",
        "np.savetxt('/content/drive/MyDrive/colab_exp_result/kaggle_data/min_multi_inputs.txt', min_m.numpy())"
      ],
      "metadata": {
        "id": "vSZvmRLeFs3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In stead of calculating min max of the input, we can read it everytime from a saved location. This will save time in terms of rerunning the min-max finding algorithm."
      ],
      "metadata": {
        "id": "9IGahACWsL8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_multi = np.float32(np.loadtxt('/content/drive/MyDrive/colab_exp_result/kaggle_data/max_multi_inputs.txt'))\n",
        "min_multi = np.float32(np.loadtxt('/content/drive/MyDrive/colab_exp_result/kaggle_data/min_multi_inputs.txt'))\n",
        "print(max_multi.shape)\n",
        "print(min_multi.shape)"
      ],
      "metadata": {
        "id": "OoBntWye8_v5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}