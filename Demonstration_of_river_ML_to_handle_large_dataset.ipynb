{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1us73KcUt99DwBncIPh4V24uxMm4Mrurn",
      "authorship_tag": "ABX9TyPtOLCSmLvuNckX6MeEJ3Oy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaswataJash/LargeDatasetHandling/blob/master/Demonstration_of_river_ML_to_handle_large_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#downloading kaggle competitions files"
      ],
      "metadata": {
        "id": "pDBY3pCeaMgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle==1.5.12"
      ],
      "metadata": {
        "id": "fmJBzixSfGEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmxlJwRZbYtA"
      },
      "outputs": [],
      "source": [
        "%%python\n",
        "\n",
        "import sys\n",
        "import logging\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s:%(levelname)s:%(message)s')\n",
        "logger = logging.getLogger('my_logger')\n",
        "#handling of kaggle interaction\n",
        "try:\n",
        "    os.environ[\"KAGGLE_CONFIG_DIR\"] = '/home' #kaggle.json file should be uploaded to /home location before executing this cell\n",
        "    kaggle_write_cmd = \"kaggle competitions download -c open-problems-multimodal\"\n",
        "    kaggle_write_call = subprocess.run(kaggle_write_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    logger.info(kaggle_write_call.stdout)\n",
        "    if kaggle_write_call.returncode != 0:\n",
        "        logger.error(\"Error in kaggle download, errorcode=%s\", kaggle_write_call.returncode)\n",
        "        sys.stdout.flush()\n",
        "        sys.exit(\"Forceful exit as kaggle download returned error\")\n",
        "except BaseException as err:\n",
        "    logger.error(\"kaggle download related error\", exc_info=True)\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while kaggle download\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/colab_exp_result/kaggle_data\n",
        "!unzip /content/open-problems-multimodal.zip -d /content/drive/MyDrive/colab_exp_result/kaggle_data"
      ],
      "metadata": {
        "id": "TT-gWlUYCG7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can mount Google drive in colab and can copy the kaggle competitions files there. This will help not to run kaggle download code everytime before start of the notebook - it can save lot of time. Instead, everytime we can directly copy the contents from drive into the local filesystem of the underneath VM hosting the notebook."
      ],
      "metadata": {
        "id": "-tmhnBezqNTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup cp /content/drive/MyDrive/colab_exp_result/kaggle_data/* /mnt &"
      ],
      "metadata": {
        "id": "e_jqgtMpmsLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /mnt"
      ],
      "metadata": {
        "id": "C77_Ud1igRen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installation of required software packages"
      ],
      "metadata": {
        "id": "4Pu-jAqrasbR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybnQEUgTlqg-"
      },
      "outputs": [],
      "source": [
        "!pip install river"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py==3.7.0"
      ],
      "metadata": {
        "id": "U5wZBh7dyDZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdf5plugin~=2.0"
      ],
      "metadata": {
        "id": "GsjShaVYyIg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HDF5 handling common code"
      ],
      "metadata": {
        "id": "upQT9S7Ea3tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hdf5_dataset_value_key(hdf5_file, debug = 0):\n",
        "    groups = []\n",
        "    def node_visit(name):\n",
        "        groups.append(name)\n",
        "    \n",
        "    hdf5_file.visit(node_visit)\n",
        "    if debug>0: print(hdf5_file, groups)\n",
        "    \n",
        "    for g in groups:\n",
        "        shape = hdf5_file[g].shape if isinstance(hdf5_file[g], h5py._hl.dataset.Dataset) else None\n",
        "        if debug>0: print(g, type(hdf5_file[g]), shape)\n",
        "        if (not shape is None) and (len(shape) == 2):\n",
        "            return g\n",
        "    \n",
        "    return None\n",
        "\n",
        "def get_hdf5_dataset_with_specific_shape(hdf5_file, size, debug = 0):\n",
        "    groups = []\n",
        "    def node_visit(name):\n",
        "        groups.append(name)\n",
        "    \n",
        "    hdf5_file.visit(node_visit)\n",
        "    if debug>0: print(hdf5_file, groups)\n",
        "    \n",
        "    for g in groups:\n",
        "        shape = hdf5_file[g].shape if isinstance(hdf5_file[g], h5py._hl.dataset.Dataset) else None\n",
        "        if debug>0: print(g, type(hdf5_file[g]), shape)\n",
        "        if (not shape is None) and (len(shape) == 1) and (shape[0] == size):\n",
        "            return g\n",
        "    \n",
        "    return None\n",
        "\n",
        "def get_hdf5_info(hdf5_file):\n",
        "    print('root-group file-object name:', hdf5_file.name)\n",
        "    def print_keys(gr, level):\n",
        "        keys = list(gr.keys())\n",
        "        for k in keys:\n",
        "            \n",
        "            if isinstance(gr[k], h5py._hl.group.Group):\n",
        "                print('->'*level, k, gr[k])\n",
        "                print_keys(gr[k], level + 1)\n",
        "            elif isinstance(gr[k], h5py._hl.dataset.Dataset):\n",
        "                print('->'*level, k, gr[k], 'size=', gr[k].size, 'nbytes=', gr[k].nbytes, \n",
        "                      'maxshape=', gr[k].maxshape, 'chunks=', gr[k].chunks)\n",
        "\n",
        "    print_keys(hdf5_file, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "rAqMEPw4yQwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import hdf5plugin #without importing this, decompression will not happen by h5py\n",
        "hdf5_input_file = h5py.File('/mnt/train_multi_inputs.h5') # HDF5 file\n",
        "get_hdf5_info(hdf5_input_file)\n",
        "hdf5_input_file.close()"
      ],
      "metadata": {
        "id": "8m9Y6UQNp5o9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import hdf5plugin #without importing this, decompression will not happen by h5py\n",
        "hdf5_input_file = h5py.File('/mnt/train_multi_inputs.h5') # HDF5 file\n",
        "hdf5_input_key = get_hdf5_dataset_value_key(hdf5_input_file, debug=1)\n",
        "d = hdf5_input_file[hdf5_input_key]          # Pointer on on-disk array\n",
        "print('shape:', d.shape, 'dtype:', d.dtype)  # d can be very large"
      ],
      "metadata": {
        "id": "RsVDw1_dlFFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hdf5_col_name_key = get_hdf5_dataset_with_specific_shape(hdf5_input_file, 228942, debug=1)\n",
        "cols = hdf5_input_file[hdf5_col_name_key]\n",
        "print(cols.shape)\n",
        "from tqdm import tqdm\n",
        "col_name = []\n",
        "for c_id in tqdm(range(cols.shape[0])):\n",
        "    col_name.append(str(cols[c_id], 'UTF-8'))"
      ],
      "metadata": {
        "id": "5E16JH80v7FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import hdf5plugin #without importing this, decompression will not happen by h5py\n",
        "hdf5_target_file = h5py.File('/mnt/train_multi_targets.h5') # HDF5 file\n",
        "hdf5_target_key = get_hdf5_dataset_value_key(hdf5_target_file, debug=1)\n",
        "d_target = hdf5_target_file[hdf5_target_key]          # Pointer on on-disk array\n",
        "print('shape:', d_target.shape, 'dtype:', d_target.dtype)  # d can be very large"
      ],
      "metadata": {
        "id": "tcFaEuTQ872o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hdf5_target_col_name_key = get_hdf5_dataset_with_specific_shape(hdf5_target_file, 23418, debug=1)\n",
        "target_cols = hdf5_target_file[hdf5_target_col_name_key]\n",
        "print(target_cols.shape)\n",
        "from tqdm import tqdm\n",
        "target_col_name = []\n",
        "for c_id in tqdm(range(target_cols.shape[0])):\n",
        "    target_col_name.append(str(target_cols[c_id], 'UTF-8'))"
      ],
      "metadata": {
        "id": "Vih9VAyRAWOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using river-ML to demonstrate online ML\n",
        "\n",
        "(River-ML can take significantly long time for training on large dataset as it needs to do frequent for-looping on python dictionaries - note that river-ML expects as wells internally maintains, dictionaries for records. However, it is remarkably conservative from required RAM perspective.)"
      ],
      "metadata": {
        "id": "TbHU0IoUa7OJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from river import stream\n",
        "dataset = stream.iter_array(X=d, y=d_target, feature_names=col_name, target_names=target_col_name)"
      ],
      "metadata": {
        "id": "wHeXkxpYglNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from river import preprocessing\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "mini_batch_input = []\n",
        "standard_scaler = preprocessing.StandardScaler()\n",
        "for iter_id, (X,_) in tqdm(enumerate(dataset)):\n",
        "    mini_batch_input.append(X)\n",
        "    if (iter_id > 0) and (iter_id%128 == 0):\n",
        "        df_input = pd.DataFrame(mini_batch_input)\n",
        "        standard_scaler.learn_many(df_input)\n",
        "        with open('/content/drive/MyDrive/colab_exp_result/standard_scaler.pkl', 'wb') as f:\n",
        "            pickle.dump(standard_scaler, f)\n",
        "        mini_batch_input.clear()\n",
        "        del df_input\n",
        "\n",
        "if len(mini_batch_input) > 0:\n",
        "    df_input = pd.DataFrame(mini_batch_input)\n",
        "    standard_scaler.learn_many(df_input)\n",
        "    with open('/content/drive/MyDrive/colab_exp_result/standard_scaler.pkl', 'wb') as f:\n",
        "        pickle.dump(standard_scaler, f)\n",
        "    mini_batch_input.clear()\n",
        "    del df_input\n",
        "\n",
        "del standard_scaler\n",
        "del mini_batch_input"
      ],
      "metadata": {
        "id": "akOV5R3snsMk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d408fa-65f0-4241-e212-d672519860b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "86271it [7:12:56, 13.69it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# load\n",
        "with open('/content/drive/MyDrive/colab_exp_result/standard_scaler.pkl', 'rb') as f:\n",
        "    standard_scaler = pickle.load(f)\n",
        "\n",
        "#Ref:https://riverml.xyz/0.14.0/recipes/on-hoeffding-trees/\n",
        "\n",
        "from river import tree\n",
        "from river import metrics\n",
        "from river import evaluate\n",
        "\n",
        "model = tree.iSOUPTreeRegressor(\n",
        "    max_size=2048,\n",
        "    memory_estimate_period=10,\n",
        "    stop_mem_management=True,\n",
        "    remove_poor_attrs=True\n",
        ")\n",
        "\n",
        "for iter_count, (X,y) in tqdm(enumerate(dataset)):\n",
        "    X_t = standard_scaler.transform_one(X)\n",
        "    model.learn_one(X_t, y)\n",
        "    if iter_count % 20:\n",
        "        with open('/content/drive/MyDrive/colab_exp_result/iSOUPTreeRegressor_model.pkl', 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/colab_exp_result/iSOUPTreeRegressor_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ],
      "metadata": {
        "id": "e5qcnyXoQLrR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}