{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "115aePpvdxaPthofW3Z412dd1e1QrMS9X",
      "authorship_tag": "ABX9TyPjpSzfX9cvhrp5uDbtoAk9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaswataJash/LargeDatasetHandling/blob/master/Incremental_min_max_calculation_for_large_dataset_using_Ray.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uname -a\n",
        "!python --version"
      ],
      "metadata": {
        "id": "ABv1EfkN9JmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ce9021-9ae2-44b2-b832-6435344c6a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linux 5f5882c553b5 5.10.147+ #1 SMP Sat Dec 10 16:00:40 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Python 3.9.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ2e2iY8Sb7I",
        "outputId": "52262a98-6f14-4db7-fa2c-44dd61589c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!df -h /dev/shm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNOT1CsxHpwS",
        "outputId": "6bf249ca-8d78-4836-f1d7-655a3a478a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "shm             5.7G     0  5.7G   0% /dev/shm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/7878707/how-to-unmount-a-busy-device\n",
        "#for python multipprocessor, data across child process and main process are being shared through shared memory\n",
        "#for pytorch Dataloader, shared memory requirement can be quite high\n",
        "!sudo umount -l /dev/shm/ && sudo mount -t tmpfs -o rw,nosuid,nodev,noexec,relatime,size=9G shm /dev/shm"
      ],
      "metadata": {
        "id": "ik3UCDFtQnAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#refer: https://numpy.org/doc/stable/reference/global_state.html#madvise-hugepage-on-linux\n",
        "!cat /sys/kernel/mm/transparent_hugepage/enabled\n",
        "!cat /sys/kernel/mm/transparent_hugepage/defrag\n",
        "!cat /sys/kernel/mm/transparent_hugepage/use_zero_page\n",
        "!cat /sys/kernel/mm/transparent_hugepage/hpage_pmd_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4r4JFDEV7SM",
        "outputId": "e0e9ba28-056c-4791-85b5-9f5f33c04461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "always [madvise] never\n",
            "always defer defer+madvise [madvise] never\n",
            "1\n",
            "2097152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env"
      ],
      "metadata": {
        "id": "dbm8LTIEZS5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/37890898/how-to-set-env-variable-in-jupyter-notebook\n",
        "%env NUMPY_MADVISE_HUGEPAGE=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxWhNXIJZec5",
        "outputId": "9d3b9e6f-c0aa-4560-d96f-aff476c8a8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: NUMPY_MADVISE_HUGEPAGE=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Determine total availiable GPU memory"
      ],
      "metadata": {
        "id": "gBnAWDn9AY-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ref: https://stackoverflow.com/questions/59567226/how-to-programmatically-determine-available-gpu-memory-with-tensorflow\n",
        "import subprocess as sp\n",
        "import os\n",
        "def get_gpu_memory():\n",
        "    command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
        "    try:\n",
        "        memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
        "        memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
        "        return memory_free_values[0] * 1024 * 1024 # memory_free_values[0] is in MB, thus converting into bytes\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return -1"
      ],
      "metadata": {
        "id": "7j4O2tiG7PD_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#downloading kaggle competitions files"
      ],
      "metadata": {
        "id": "pGV5IETWGxjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle==1.5.12"
      ],
      "metadata": {
        "id": "fmJBzixSfGEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmxlJwRZbYtA"
      },
      "outputs": [],
      "source": [
        "%%python\n",
        "\n",
        "import sys\n",
        "import logging\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s:%(levelname)s:%(message)s')\n",
        "logger = logging.getLogger('my_logger')\n",
        "#handling of kaggle interaction\n",
        "try:\n",
        "    os.environ[\"KAGGLE_CONFIG_DIR\"] = '/home' #kaggle.json file should be uploaded to /home location before executing this cell\n",
        "    kaggle_write_cmd = \"kaggle competitions download -c open-problems-multimodal\"\n",
        "    kaggle_write_call = subprocess.run(kaggle_write_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    logger.info(kaggle_write_call.stdout)\n",
        "    if kaggle_write_call.returncode != 0:\n",
        "        logger.error(\"Error in kaggle download, errorcode=%s\", kaggle_write_call.returncode)\n",
        "        sys.stdout.flush()\n",
        "        sys.exit(\"Forceful exit as kaggle download returned error\")\n",
        "except BaseException as err:\n",
        "    logger.error(\"kaggle download related error\", exc_info=True)\n",
        "    sys.stdout.flush()\n",
        "    sys.exit(\"Forceful exit as exception encountered while kaggle download\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/colab_exp_result/kaggle_data\n",
        "!unzip /content/open-problems-multimodal.zip -d /content/drive/MyDrive/colab_exp_result/kaggle_data"
      ],
      "metadata": {
        "id": "TT-gWlUYCG7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can mount Google drive in colab and can copy the kaggle competitions files there. This will help not to run kaggle download code everytime before start of the notebook - it can save lot of time. Instead, everytime we can directly copy the contents from drive into the local filesystem of the underneath VM hosting the notebook."
      ],
      "metadata": {
        "id": "-tmhnBezqNTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup cp /content/drive/MyDrive/colab_exp_result/kaggle_data/* /mnt &"
      ],
      "metadata": {
        "id": "e_jqgtMpmsLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a38176-e85e-486e-d08a-f61615c75cd4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /mnt"
      ],
      "metadata": {
        "id": "C77_Ud1igRen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c2c555-61da-42a6-ff85-0131899ae887"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 28181072\n",
            "-rw------- 1 root root  2418406934 Apr 20 03:14 evaluation_ids.csv\n",
            "-rw------- 1 root root      551250 Apr 20 03:14 max_cite_inputs.txt\n",
            "-rw------- 1 root root     5723550 Apr 20 03:14 max_multi_inputs.txt\n",
            "-rw------- 1 root root      234920 Apr 20 03:14 metadata_cite_day_2_donor_27678.csv\n",
            "-rw------- 1 root root     9770334 Apr 20 03:14 metadata.csv\n",
            "-rw------- 1 root root      551250 Apr 20 03:14 min_cite_inputs.txt\n",
            "-rw------- 1 root root     5723550 Apr 20 03:14 min_multi_inputs.txt\n",
            "-rw------- 1 root root   843563244 Apr 20 03:15 sample_submission.csv\n",
            "-rw------- 1 root root   307964530 Apr 20 03:15 test_cite_inputs_day_2_donor_27678.h5\n",
            "-rw------- 1 root root  1704565845 Apr 20 03:15 test_cite_inputs.h5\n",
            "-rw------- 1 root root  6473530657 Apr 20 03:16 test_multi_inputs.h5\n",
            "-rw------- 1 root root  2498128492 Apr 20 03:17 train_cite_inputs.h5\n",
            "-rw------- 1 root root    38539123 Apr 20 03:17 train_cite_targets.h5\n",
            "-rw------- 1 root root 11334840656 Apr 20 03:20 train_multi_inputs.h5\n",
            "-rw------- 1 root root  3215261538 Apr 20 03:20 train_multi_targets.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installation of required software packages"
      ],
      "metadata": {
        "id": "4Pu-jAqrasbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py==3.8.0"
      ],
      "metadata": {
        "id": "U5wZBh7dyDZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd9f69a3-6a87-4ead-ccbe-68c0f619bf86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: h5py==3.8.0 in /usr/local/lib/python3.9/dist-packages (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.9/dist-packages (from h5py==3.8.0) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ref: https://docs.h5py.org/en/stable/mpi.html\n",
        "#check whether parallel version of h5py is availiable\n",
        "!h5cc -showconfig"
      ],
      "metadata": {
        "id": "TN3pxs5zhvGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdf5plugin~=2.0"
      ],
      "metadata": {
        "id": "GsjShaVYyIg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c229cf9-ede1-44ce-aa70-fc9616e0e9ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hdf5plugin~=2.0\n",
            "  Downloading hdf5plugin-2.3.2-py2.py3-none-manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (from hdf5plugin~=2.0) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.9/dist-packages (from h5py->hdf5plugin~=2.0) (1.22.4)\n",
            "Installing collected packages: hdf5plugin\n",
            "Successfully installed hdf5plugin-2.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"ray[default]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZtP3El6_Rjs",
        "outputId": "1ad22205-57b8-4037-9d06-26f0d4082a14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ray[default]\n",
            "  Downloading ray-2.3.1-cp39-cp39-manylinux2014_x86_64.whl (58.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from ray[default]) (8.1.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/dist-packages (from ray[default]) (4.3.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.9/dist-packages (from ray[default]) (23.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from ray[default]) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.9/dist-packages (from ray[default]) (1.22.4)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ray[default]) (1.0.5)\n",
            "Collecting aiosignal\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting frozenlist\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from ray[default]) (6.0)\n",
            "Collecting virtualenv>=20.0.24\n",
            "  Downloading virtualenv-20.22.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.9/dist-packages (from ray[default]) (3.20.3)\n",
            "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.9/dist-packages (from ray[default]) (1.53.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from ray[default]) (3.11.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/dist-packages (from ray[default]) (1.10.7)\n",
            "Collecting aiohttp-cors\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting gpustat>=1.0.0\n",
            "  Downloading gpustat-1.1.tar.gz (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting colorful\n",
            "  Downloading colorful-0.5.5-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from ray[default]) (0.16.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.9/dist-packages (from ray[default]) (6.3.0)\n",
            "Collecting aiohttp>=3.7\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencensus\n",
            "  Downloading opencensus-0.11.2-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py-spy>=0.2.0\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp>=3.7->ray[default]) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blessed>=1.17.1\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.9/dist-packages (from gpustat>=1.0.0->ray[default]) (5.9.5)\n",
            "Collecting nvidia-ml-py>=11.450.129\n",
            "  Downloading nvidia_ml_py-11.525.112-py3-none-any.whl (35 kB)\n",
            "Collecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4,>=3.2 in /usr/local/lib/python3.9/dist-packages (from virtualenv>=20.0.24->ray[default]) (3.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema->ray[default]) (0.19.3)\n",
            "Collecting opencensus-context>=0.1.3\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from opencensus->ray[default]) (2.11.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic->ray[default]) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->ray[default]) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->ray[default]) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->ray[default]) (3.4)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.9/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]) (0.2.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.14.1 in /usr/local/lib/python3.9/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (2.17.3)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (5.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.4.8)\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.1-py3-none-any.whl size=26298 sha256=15eaa45ec7b66079e90d32e6a72827ff2568eb161987fc208fa17f8459cbfb78\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/f0/b3/8566d6821307110981a5db015cbf8fd88697446f81e5f40a27\n",
            "Successfully built gpustat\n",
            "Installing collected packages: py-spy, opencensus-context, nvidia-ml-py, distlib, colorful, virtualenv, multidict, frozenlist, blessed, async-timeout, yarl, gpustat, aiosignal, ray, aiohttp, opencensus, aiohttp-cors\n",
            "Successfully installed aiohttp-3.8.4 aiohttp-cors-0.7.0 aiosignal-1.3.1 async-timeout-4.0.2 blessed-1.20.0 colorful-0.5.5 distlib-0.3.6 frozenlist-1.3.3 gpustat-1.1 multidict-6.0.4 nvidia-ml-py-11.525.112 opencensus-0.11.2 opencensus-context-0.1.3 py-spy-0.3.14 ray-2.3.1 virtualenv-20.22.0 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#HDF5 handling common code"
      ],
      "metadata": {
        "id": "upQT9S7Ea3tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import hdf5plugin #without importing this, decompression will not happen by h5py\n",
        "def get_hdf5_dataset_value_key(hdf5_file, debug = 0):\n",
        "    groups = []\n",
        "    def node_visit(name):\n",
        "        groups.append(name)\n",
        "    \n",
        "    hdf5_file.visit(node_visit)\n",
        "    if debug>0: print(hdf5_file, groups)\n",
        "    \n",
        "    for g in groups:\n",
        "        shape = hdf5_file[g].shape if isinstance(hdf5_file[g], h5py._hl.dataset.Dataset) else None\n",
        "        if debug>0: print(g, type(hdf5_file[g]), shape)\n",
        "        if (not shape is None) and (len(shape) == 2):\n",
        "            return g\n",
        "    \n",
        "    return None\n",
        "\n",
        "def get_hdf5_dataset_with_specific_shape(hdf5_file, size, debug = 0):\n",
        "    groups = []\n",
        "    def node_visit(name):\n",
        "        groups.append(name)\n",
        "    \n",
        "    hdf5_file.visit(node_visit)\n",
        "    if debug>0: print(hdf5_file, groups)\n",
        "    \n",
        "    for g in groups:\n",
        "        shape = hdf5_file[g].shape if isinstance(hdf5_file[g], h5py._hl.dataset.Dataset) else None\n",
        "        if debug>0: print(g, type(hdf5_file[g]), shape)\n",
        "        if (not shape is None) and (len(shape) == 1) and (shape[0] == size):\n",
        "            return g\n",
        "    \n",
        "    return None\n",
        "\n",
        "def get_hdf5_info(hdf5_file):\n",
        "    print('root-group file-object name:', hdf5_file.name)\n",
        "    def print_keys(gr, level):\n",
        "        keys = list(gr.keys())\n",
        "        for k in keys:\n",
        "            \n",
        "            if isinstance(gr[k], h5py._hl.group.Group):\n",
        "                print('->'*level, k, gr[k])\n",
        "                print_keys(gr[k], level + 1)\n",
        "            elif isinstance(gr[k], h5py._hl.dataset.Dataset):\n",
        "                print('->'*level, k, gr[k], 'dtype=', gr[k].dtype , 'size=', gr[k].size, 'nbytes=', gr[k].nbytes, \n",
        "                      'maxshape=', gr[k].maxshape, 'chunks=', gr[k].chunks)\n",
        "\n",
        "    print_keys(hdf5_file, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "rAqMEPw4yQwL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import hdf5plugin #without importing this, decompression will not happen by h5py\n",
        "print('============= TRAIN MULTI INPUT ====================')\n",
        "train_multi_input_file = h5py.File('/mnt/train_multi_inputs.h5') # HDF5 file\n",
        "get_hdf5_info(train_multi_input_file)\n",
        "train_multi_input_file.close()\n",
        "del train_multi_input_file\n",
        "print('============= TEST MULTI INPUT ====================')\n",
        "test_multi_input_file = h5py.File('/mnt/test_multi_inputs.h5') # HDF5 file\n",
        "get_hdf5_info(test_multi_input_file)\n",
        "test_multi_input_file.close()\n",
        "del test_multi_input_file"
      ],
      "metadata": {
        "id": "8m9Y6UQNp5o9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2cea861-f5b3-484d-ad9b-bf4918a18541"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= TRAIN MULTI INPUT ====================\n",
            "root-group file-object name: /\n",
            "-> train_multi_inputs <HDF5 group \"/train_multi_inputs\" (4 members)>\n",
            "->-> axis0 <HDF5 dataset \"axis0\": shape (228942,), type \"|S26\"> dtype= |S26 size= 228942 nbytes= 5952492 maxshape= (228942,) chunks= (2520,)\n",
            "->-> axis1 <HDF5 dataset \"axis1\": shape (105942,), type \"|S12\"> dtype= |S12 size= 105942 nbytes= 1271304 maxshape= (105942,) chunks= (5461,)\n",
            "->-> block0_items <HDF5 dataset \"block0_items\": shape (228942,), type \"|S26\"> dtype= |S26 size= 228942 nbytes= 5952492 maxshape= (228942,) chunks= (2520,)\n",
            "->-> block0_values <HDF5 dataset \"block0_values\": shape (105942, 228942), type \"<f4\"> dtype= float32 size= 24254573364 nbytes= 97018293456 maxshape= (105942, 228942) chunks= (1, 228942)\n",
            "============= TEST MULTI INPUT ====================\n",
            "root-group file-object name: /\n",
            "-> test_multi_inputs <HDF5 group \"/test_multi_inputs\" (4 members)>\n",
            "->-> axis0 <HDF5 dataset \"axis0\": shape (228942,), type \"|S26\"> dtype= |S26 size= 228942 nbytes= 5952492 maxshape= (228942,) chunks= (2520,)\n",
            "->-> axis1 <HDF5 dataset \"axis1\": shape (55935,), type \"|S12\"> dtype= |S12 size= 55935 nbytes= 671220 maxshape= (55935,) chunks= (5461,)\n",
            "->-> block0_items <HDF5 dataset \"block0_items\": shape (228942,), type \"|S26\"> dtype= |S26 size= 228942 nbytes= 5952492 maxshape= (228942,) chunks= (2520,)\n",
            "->-> block0_values <HDF5 dataset \"block0_values\": shape (55935, 228942), type \"<f4\"> dtype= float32 size= 12805870770 nbytes= 51223483080 maxshape= (55935, 228942) chunks= (1, 228942)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import hdf5plugin #without importing this, decompression will not happen by h5py\n",
        "train_mult_input_file = h5py.File('/mnt/train_multi_inputs.h5') # HDF5 file\n",
        "hdf5_input_key = get_hdf5_dataset_value_key(train_mult_input_file, debug=1)"
      ],
      "metadata": {
        "id": "RsVDw1_dlFFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746ec49b-7855-493a-ce8d-bc2ffb7c6c13"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<HDF5 file \"train_multi_inputs.h5\" (mode r)> ['train_multi_inputs', 'train_multi_inputs/axis0', 'train_multi_inputs/axis1', 'train_multi_inputs/block0_items', 'train_multi_inputs/block0_values']\n",
            "train_multi_inputs <class 'h5py._hl.group.Group'> None\n",
            "train_multi_inputs/axis0 <class 'h5py._hl.dataset.Dataset'> (228942,)\n",
            "train_multi_inputs/axis1 <class 'h5py._hl.dataset.Dataset'> (105942,)\n",
            "train_multi_inputs/block0_items <class 'h5py._hl.dataset.Dataset'> (228942,)\n",
            "train_multi_inputs/block0_values <class 'h5py._hl.dataset.Dataset'> (105942, 228942)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hdf5_col_name_key = get_hdf5_dataset_with_specific_shape(train_mult_input_file, 228942, debug=1)\n",
        "cols = train_mult_input_file[hdf5_col_name_key]\n",
        "print(cols.shape)\n",
        "from tqdm import tqdm\n",
        "col_name = []\n",
        "for c_id in tqdm(range(cols.shape[0])):\n",
        "    col_name.append(str(cols[c_id], 'UTF-8'))"
      ],
      "metadata": {
        "id": "5E16JH80v7FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "nGHPADV_4Aq8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   https://luis-sena.medium.com/sharing-big-numpy-arrays-across-python-processes-abf0dc2a0ab2 (why ray with shared object store is best sol)\n",
        "*   Ref: https://towardsdatascience.com/histogram-on-function-space-4a710241f026\n",
        "*   Ref: https://stackoverflow.com/questions/71844846/is-there-a-faster-way-to-get-correlation-coefficents (fast corr-coef)\n",
        "\n"
      ],
      "metadata": {
        "id": "4aadQ-lstUfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global min and max determination of the raw-inputs (will be used for min-max normalization of the data)"
      ],
      "metadata": {
        "id": "gbP33e4B293Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import traceback\n",
        "import ray\n",
        "\n",
        "#@ray.remote(num_cpus=0.5, num_gpus=0.25)\n",
        "#@ray.remote(num_cpus=0.5)\n",
        "@ray.remote\n",
        "class RawInputDataset:\n",
        "\n",
        "    import h5py\n",
        "    import hdf5plugin #without importing this, decompression will not happen by h5py\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    def __init__(self, hdf5_input_path, batch_size=1, debug=0):\n",
        "        self.inited = False\n",
        "        if debug>0: print('hdf5_input_path:', hdf5_input_path, 'batch_size:', batch_size, 'debug:', debug)\n",
        "\n",
        "        assert batch_size >= 1\n",
        "\n",
        "        self.hdf5_input_path = hdf5_input_path\n",
        "        self.batch_size=batch_size\n",
        "        self.debug = debug\n",
        "        \n",
        "        self.hdf5_input = h5py.File(self.hdf5_input_path, 'r', driver='stdio')\n",
        "        hdf5_input_key = get_hdf5_dataset_value_key(self.hdf5_input)\n",
        "        self.hdf5_dataset = self.hdf5_input[hdf5_input_key]\n",
        "        self.len = self.hdf5_dataset.shape[0]\n",
        "\n",
        "        self.cuda_device = torch.device(\"cuda:0\" if torch.cuda.is_available and (get_gpu_memory() > 0) else \"cpu\")\n",
        "        self.input = np.zeros((batch_size,self.hdf5_dataset.shape[1]), dtype=self.hdf5_dataset.dtype)\n",
        "        self.stat_id_consumed = []\n",
        "        if self.debug>0: print('hdf5 file:', self.hdf5_input, 'hdf5 group:', hdf5_input_key, 'hdf5 dataset:', self.hdf5_dataset, 'id=', id(self.hdf5_input), flush=True)\n",
        "        if self.debug>0: print('torch-device:', self.cuda_device, 'self.input:', self.input.shape)\n",
        "        self.inited =True\n",
        "\n",
        "    def is_inited(self):\n",
        "        return self.inited\n",
        "    \n",
        "    # without '__reduce__', the instance is unserializable.\n",
        "    '''\n",
        "    def __reduce__(self):\n",
        "        deserializer = RawInputDataset\n",
        "        serialized_data = (self.hdf5_input_path, self.batch_size, self.debug)\n",
        "        return deserializer, serialized_data\n",
        "    '''\n",
        "\n",
        "    def __len__(self): return self.len   \n",
        "\n",
        "    def __getitem__(self, row):\n",
        "        try:\n",
        "            assert row < self.len\n",
        "            input = self.hdf5_dataset[row]\n",
        "            if self.debug>0:\n",
        "                self.stat_id_consumed.append(row)\n",
        "            if self.debug >4: print('type of input=', type(input) , 'shape=', input.shape, flush=True)\n",
        "            return torch.from_numpy(input).detach().to(self.cuda_device)\n",
        "        except Exception as e:\n",
        "            print('Exception occurred in __getitem__:', e)\n",
        "            traceback.print_exc()\n",
        "            \n",
        "        return None\n",
        "\n",
        "    def get_batch(self, starting_row):\n",
        "        try:\n",
        "            assert starting_row < self.len\n",
        "            end_row = min(starting_row + self.batch_size, self.len)            \n",
        "            #input = self.hdf5_dataset[starting_row:end_row]\n",
        "            input = self.input\n",
        "            if input.shape[0] != (end_row - starting_row): #will happen for the last batch\n",
        "                input = np.zeros(((end_row - starting_row),self.hdf5_dataset.shape[1]), dtype=self.hdf5_dataset.dtype)\n",
        "            self.hdf5_dataset.read_direct(input, source_sel=np.s_[starting_row:end_row,:], dest_sel=None)\n",
        "            if self.debug>0:\n",
        "                self.stat_id_consumed.extend(range(starting_row, end_row))\n",
        "            if self.debug >4: print('type of input=', type(input) , 'shape=', input.shape, flush=True)\n",
        "            return torch.from_numpy(input).detach().to(self.cuda_device)\n",
        "        except Exception as e:\n",
        "            print('Exception occurred in get_batch():', e)\n",
        "            traceback.print_exc()\n",
        "            \n",
        "        return None\n",
        "\n",
        "    def find_min_max_on_batch(self, starting_row):\n",
        "        try:\n",
        "            data = self.get_batch(starting_row)\n",
        "            local_min = torch.min(data, dim=0)[0] #we have to find min for each col (so reduction of dim=0)\n",
        "            local_max = torch.max(data, dim=0)[0] #we have to find max for each col (so reduction of dim=0)    \n",
        "            return (local_min, local_max)\n",
        "        except Exception as e:\n",
        "            print('Exception occurred in find_min_max_on_batch():', e)\n",
        "            traceback.print_exc()\n",
        "\n",
        "        return (None, None)\n",
        "\n",
        "    def reset_stat(self):\n",
        "        if self.debug <= 0:\n",
        "           return\n",
        "        self.stat_id_consumed.clear()    \n",
        "\n",
        "    def __del__(self):\n",
        "        self.hdf5_input.close()\n"
      ],
      "metadata": {
        "id": "kh15dnhSbgTx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install memray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAbGnW3CAzm5",
        "outputId": "84ab39b7-cad5-4d33-fd9b-7ef360196ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting memray\n",
            "  Downloading memray-1.7.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=11.2.0 in /usr/local/lib/python3.9/dist-packages (from memray) (13.3.3)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.9/dist-packages (from memray) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2>=2.9->memray) (2.1.2)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich>=11.2.0->memray) (2.14.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich>=11.2.0->memray) (2.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.2.0->memray) (0.1.2)\n",
            "Installing collected packages: memray\n",
            "Successfully installed memray-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext memray"
      ],
      "metadata": {
        "id": "vgv-oWy1A_eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%memray_flamegraph --trace-python-allocators --follow-fork --native --leaks \n",
        "import math\n",
        "import traceback\n",
        "import gc\n",
        "import os\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "\n",
        "import ray\n",
        "\n",
        "def find_min_max():\n",
        "    try:\n",
        "        hdf5_input_file_path = '/mnt/train_multi_inputs.h5'\n",
        "        hdf5_input = h5py.File(hdf5_input_file_path, 'r')\n",
        "        hdf5_input_key = get_hdf5_dataset_value_key(hdf5_input)\n",
        "        data_len = hdf5_input[hdf5_input_key].shape[0]\n",
        "        first_elem = hdf5_input[hdf5_input_key][0]\n",
        "        elem_size_in_bytes = first_elem.size * first_elem.itemsize\n",
        "        print('first_elem.size:', first_elem.size, 'first_elem.itemsize:', first_elem.itemsize, 'elem_size_in_bytes:', elem_size_in_bytes)\n",
        "        hdf5_input.close()\n",
        "        del hdf5_input\n",
        "\n",
        "        DEBUG_LEVEL = 1\n",
        "\n",
        "        #optimal_batch_size = math.floor((1024 * 1024 * 1024) / elem_size_in_bytes) #max 1GB of numpy array \n",
        "        optimal_batch_size = math.floor((20 * 1024 * 1024) / elem_size_in_bytes) #max 20MB of numpy array \n",
        "        print('optimal_batch_size:', optimal_batch_size)\n",
        "\n",
        "        print('number of cpu availiable:', os.cpu_count())\n",
        "\n",
        "        while(gc.collect() > 0): pass #clean the memory as much as possible\n",
        "\n",
        "        work_arg = []\n",
        "        for s_row in range(0, data_len, optimal_batch_size):\n",
        "        #for s_row in range(0, 6 * optimal_batch_size, optimal_batch_size):\n",
        "            work_arg.append(s_row)\n",
        "        print(\"total task required:\", len(work_arg))\n",
        "        \n",
        "        min_max_actors = [RawInputDataset.remote(hdf5_input_file_path, optimal_batch_size, DEBUG_LEVEL) for _ in range(os.cpu_count() * 4)]\n",
        "        \n",
        "        for actor in min_max_actors:\n",
        "            while(not ray.get(actor.is_inited.remote())): pass\n",
        "            print(actor, 'is initialized')\n",
        "\n",
        "        result_ids = []\n",
        "        for id, w in enumerate(work_arg):\n",
        "            result_ids.append(min_max_actors[id%len(min_max_actors)].find_min_max_on_batch.remote(w))\n",
        "\n",
        "        min = None\n",
        "        max = None\n",
        "        with tqdm(total=len(result_ids)) as pbar:\n",
        "            while len(result_ids):\n",
        "                done_ids, result_ids = ray.wait(result_ids)\n",
        "                for d in done_ids:\n",
        "                    result = ray.get(d)\n",
        "                    if not (min is None):\n",
        "                        min = torch.minimum(min, result[0])\n",
        "                        max = torch.maximum(max, result[1])\n",
        "                    else:\n",
        "                        min = result[0]\n",
        "                        max = result[1]\n",
        "                    pbar.update(1)     \n",
        "            \n",
        "        print('max.shape:', max.shape, 'min.shape:', min.shape)\n",
        "        return max, min\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        traceback.print_exc()\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    print('MAIN pid=', os. getpid())\n",
        "\n",
        "    ray.shutdown()\n",
        "    ray.init()\n",
        "    assert ray.is_initialized()\n",
        "\n",
        "    find_min_max()\n",
        "    gc.collect()\n",
        "    print(gc.get_stats())\n",
        "\n",
        "    ray.shutdown()\n",
        "    assert not ray.is_initialized()"
      ],
      "metadata": {
        "id": "t6Lcq3D7qB__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b74da9-b2ad-4922-ad8d-86018bc65199"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAIN pid= 154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-04-20 07:16:27,890\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first_elem.size: 228942 first_elem.itemsize: 4 elem_size_in_bytes: 915768\n",
            "optimal_batch_size: 22\n",
            "number of cpu availiable: 2\n",
            "total task required: 4816\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67780)\u001b[0m hdf5_input_path: /mnt/train_multi_inputs.h5 batch_size: 22 debug: 1\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67781)\u001b[0m hdf5_input_path: /mnt/train_multi_inputs.h5 batch_size: 22 debug: 1\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67839)\u001b[0m hdf5_input_path: /mnt/train_multi_inputs.h5 batch_size: 22 debug: 1\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67839)\u001b[0m [Errno 2] No such file or directory: 'nvidia-smi'\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67839)\u001b[0m hdf5 file: <HDF5 file \"train_multi_inputs.h5\" (mode r)> hdf5 group: train_multi_inputs/block0_values hdf5 dataset: <HDF5 dataset \"block0_values\": shape (105942, 228942), type \"<f4\"> id= 140403600037872\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67839)\u001b[0m torch-device: cpu self.input: (22, 228942)\n",
            "Actor(RawInputDataset, cde97d62c2900db299574ef801000000) is initialized\n",
            "Actor(RawInputDataset, f152891864f7704e8415553b01000000) is initialized\n",
            "Actor(RawInputDataset, f9d1d882834fdf9d629525ca01000000) is initialized\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67780)\u001b[0m [Errno 2] No such file or directory: 'nvidia-smi'\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67780)\u001b[0m hdf5 file: <HDF5 file \"train_multi_inputs.h5\" (mode r)> hdf5 group: train_multi_inputs/block0_values hdf5 dataset: <HDF5 dataset \"block0_values\": shape (105942, 228942), type \"<f4\"> id= 140440443103216\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67780)\u001b[0m torch-device: cpu self.input: (22, 228942)\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67781)\u001b[0m [Errno 2] No such file or directory: 'nvidia-smi'\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67781)\u001b[0m hdf5 file: <HDF5 file \"train_multi_inputs.h5\" (mode r)> hdf5 group: train_multi_inputs/block0_values hdf5 dataset: <HDF5 dataset \"block0_values\": shape (105942, 228942), type \"<f4\"> id= 139956957276144\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67781)\u001b[0m torch-device: cpu self.input: (22, 228942)\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67901)\u001b[0m hdf5_input_path: /mnt/train_multi_inputs.h5 batch_size: 22 debug: 1\n",
            "Actor(RawInputDataset, e52fbaccf92a2170be65459401000000) is initialized\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67901)\u001b[0m [Errno 2] No such file or directory: 'nvidia-smi'\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67901)\u001b[0m hdf5 file: <HDF5 file \"train_multi_inputs.h5\" (mode r)> hdf5 group: train_multi_inputs/block0_values hdf5 dataset: <HDF5 dataset \"block0_values\": shape (105942, 228942), type \"<f4\"> id= 140377264801776\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67901)\u001b[0m torch-device: cpu self.input: (22, 228942)\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67957)\u001b[0m hdf5_input_path: /mnt/train_multi_inputs.h5 batch_size: 22 debug: 1\n",
            "Actor(RawInputDataset, 63c0c7eb364421b2aa736d0501000000) is initialized\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67957)\u001b[0m [Errno 2] No such file or directory: 'nvidia-smi'\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67957)\u001b[0m hdf5 file: <HDF5 file \"train_multi_inputs.h5\" (mode r)> hdf5 group: train_multi_inputs/block0_values hdf5 dataset: <HDF5 dataset \"block0_values\": shape (105942, 228942), type \"<f4\"> id= 139733720025072\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=67957)\u001b[0m torch-device: cpu self.input: (22, 228942)\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=68055)\u001b[0m hdf5_input_path: /mnt/train_multi_inputs.h5 batch_size: 22 debug: 1\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=68055)\u001b[0m [Errno 2] No such file or directory: 'nvidia-smi'\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=68055)\u001b[0m hdf5 file: <HDF5 file \"train_multi_inputs.h5\" (mode r)> hdf5 group: train_multi_inputs/block0_values hdf5 dataset: <HDF5 dataset \"block0_values\": shape (105942, 228942), type \"<f4\"> id= 140657799407360\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=68055)\u001b[0m torch-device: cpu self.input: (22, 228942)\n",
            "Actor(RawInputDataset, 878f723eb6e83b59176fcd0401000000) is initialized\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=68116)\u001b[0m hdf5_input_path: /mnt/train_multi_inputs.h5 batch_size: 22 debug: 1\n",
            "Actor(RawInputDataset, 06926fa9f1ee3afea50be79501000000) is initialized\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=68116)\u001b[0m [Errno 2] No such file or directory: 'nvidia-smi'\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=68116)\u001b[0m hdf5 file: <HDF5 file \"train_multi_inputs.h5\" (mode r)> hdf5 group: train_multi_inputs/block0_values hdf5 dataset: <HDF5 dataset \"block0_values\": shape (105942, 228942), type \"<f4\"> id= 140170748147696\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=68116)\u001b[0m torch-device: cpu self.input: (22, 228942)\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=68156)\u001b[0m hdf5_input_path: /mnt/train_multi_inputs.h5 batch_size: 22 debug: 1\n",
            "Actor(RawInputDataset, ef46c3931a41b0d470d4514801000000) is initialized\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=68156)\u001b[0m [Errno 2] No such file or directory: 'nvidia-smi'\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=68156)\u001b[0m hdf5 file: <HDF5 file \"train_multi_inputs.h5\" (mode r)> hdf5 group: train_multi_inputs/block0_values hdf5 dataset: <HDF5 dataset \"block0_values\": shape (105942, 228942), type \"<f4\"> id= 139939505633024\n",
            "\u001b[2m\u001b[36m(RawInputDataset pid=68156)\u001b[0m torch-device: cpu self.input: (22, 228942)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4816/4816 [05:51<00:00, 13.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max.shape: torch.Size([228942]) min.shape: torch.Size([228942])\n",
            "[{'collections': 1073, 'collected': 167741, 'uncollectable': 0}, {'collections': 86, 'collected': 16065, 'uncollectable': 0}, {'collections': 48, 'collected': 2973, 'uncollectable': 0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt('/content/drive/MyDrive/colab_exp_result/kaggle_data/max_multi_inputs.txt', max_m.numpy())\n",
        "np.savetxt('/content/drive/MyDrive/colab_exp_result/kaggle_data/min_multi_inputs.txt', min_m.numpy())"
      ],
      "metadata": {
        "id": "vSZvmRLeFs3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In stead of calculating min max of the input, we can read it everytime from a saved location. This will save time in terms of rerunning the min-max finding algorithm."
      ],
      "metadata": {
        "id": "9IGahACWsL8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_multi = np.float32(np.loadtxt('/content/drive/MyDrive/colab_exp_result/kaggle_data/max_multi_inputs.txt'))\n",
        "min_multi = np.float32(np.loadtxt('/content/drive/MyDrive/colab_exp_result/kaggle_data/min_multi_inputs.txt'))\n",
        "print(max_multi.shape)\n",
        "print(min_multi.shape)"
      ],
      "metadata": {
        "id": "OoBntWye8_v5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}